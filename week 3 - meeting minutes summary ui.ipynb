{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Converting speech to text through automatic speech recognition pipeline"
      ],
      "metadata": {
        "id": "4qnehPG7006j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installs\n",
        "!pip install -q transformers torch safetensors accelerate bitsandbytes\n",
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline,AutoModelForCausalLM,AutoTokenizer,TextStreamer,BitsAndBytesConfig"
      ],
      "metadata": {
        "id": "V3fmQqbb1K2U"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbls307I1XeW",
        "outputId": "67e124d1-273e-4dc3-8e9b-3d72cedf0766"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_MODEL = \"openai/whisper-medium\"\n",
        "speech_model = AutoModelForSpeechSeq2Seq.from_pretrained(AUDIO_MODEL, torch_dtype=torch.float16, low_cpu_mem_usage=True, use_safetensors=True)\n",
        "speech_model.to('cuda')\n",
        "processor = AutoProcessor.from_pretrained(AUDIO_MODEL)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=speech_model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    torch_dtype=torch.float16,\n",
        "    device='cuda',\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyBnne9B1X4U",
        "outputId": "86632e86-c492-4f51-f856-1719ac82e9ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarising the text through a second pipeline"
      ],
      "metadata": {
        "id": "l6QhWQbm5bFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "5H2cjGFL5m0O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\""
      ],
      "metadata": {
        "id": "qVKpUAEA8kuQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "# from transformers import pipeline\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "# ]\n",
        "# pipe = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)\n",
        "# pipe(messages)"
      ],
      "metadata": {
        "id": "VAUQnWBa6HOI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def messages_for(audio_title,transcription):\n",
        "  messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"},\n",
        "      {\"role\": \"user\", \"content\": f\"Below is an extract transcript of a {audio_title}. Please write minutes in markdown, including a summary with attendees, location and date; discussion points; takeaways; and action items with owners.\\n{transcription}\"},\n",
        "  ]\n",
        "  return messages"
      ],
      "metadata": {
        "id": "v9xZwskP6uD9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quant_config():\n",
        "  return BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "54ndCr3G83g4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, messages):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config(), trust_remote_code=True)\n",
        "    outputs = model.generate(inputs, max_new_tokens=80)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Mbqygznv9Lv1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "yR4OugfZIW7d"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio_title=\"\",audio_file=None):\n",
        "    \"\"\"Transcribes audio and summarizes it into meeting minutes.\"\"\"\n",
        "\n",
        "    # Step 1: Transcribe the audio\n",
        "    transcription = pipe(audio_file,return_timestamps=True)[\"text\"]\n",
        "\n",
        "    # Step 2: Create messages list, quantize, generate summary\n",
        "    messages = messages_for(audio_title,transcription)\n",
        "    summary = generate(model,messages)\n",
        "\n",
        "    # Fixing o/p format\n",
        "    match = re.search(r\"minutes of the\", summary, re.IGNORECASE)\n",
        "    if match:\n",
        "        summary = summary[match.start():]\n",
        "\n",
        "    return transcription, summary"
      ],
      "metadata": {
        "id": "AteKtjrt6Zul"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q gradio\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "K-KImkrs9p8r"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "force_dark_mode = \"\"\"\n",
        "function refresh() {\n",
        "    const url = new URL(window.location);\n",
        "    if (url.searchParams.get('__theme') !== 'dark') {\n",
        "        url.searchParams.set('__theme', 'dark');\n",
        "        window.location.href = url.href;\n",
        "    }\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0Zs7rRBJhQ4V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=process_audio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Audio Title\"),  # New input for audio title\n",
        "        gr.Audio(type=\"filepath\")  # Accepts an audio file\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Transcription\"),\n",
        "        gr.Markdown()\n",
        "    ],\n",
        "    title=\"AI Meeting Minutes Generator\",\n",
        "    description=\"Upload an audio file or enable microphone, and this tool will convert speech to text and summarize it into meeting minutes.\",\n",
        "    js=force_dark_mode,\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "# Launch the Interface\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eoAkwSw3lIwf",
        "outputId": "6d58a3d4-dad7-4cd6-d9bb-ca81957aa2c3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://ec71eb0cd52f2cc41f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ec71eb0cd52f2cc41f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2103, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1650, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-d0d0dfae1c04>\", line 9, in process_audio\n",
            "    summary = generate(model,messages)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-d59c5eed63dd>\", line 5, in generate\n",
            "    model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config(), trust_remote_code=True)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
            "    return model_class.from_pretrained(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\", line 4188, in from_pretrained\n",
            "    hf_quantizer.validate_environment(device_map=device_map)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 103, in validate_environment\n",
            "    raise ValueError(\n",
            "ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://ec71eb0cd52f2cc41f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}