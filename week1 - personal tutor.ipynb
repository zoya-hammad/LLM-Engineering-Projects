{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Personal Tutor\n",
    "\n",
    "To demonstrate familiarity with OpenAI API, and also Ollama, this tool that takes a technical question,  \n",
    "and responds with an explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78251e41-ddd2-4351-a71c-2c87f5cfc8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 74701a8c35f6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.3 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 4f659a1e86d7... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  485 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ead55d-fd2f-49c8-9a16-c86e6b792f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_system_prompt = \"You are a personal study tutor, designed to provide clear, yet brief and succint answers to \\\n",
    "students that ask you questions. The topics are related to data science, computer science \\\n",
    "and technology in general, so you are allowed to use a moderate level of jargon. Explain in \\\n",
    "simple terminology, so a student can easily understand. Use a tabular format where possible \\\n",
    "for ease of information flow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "openai.chat.completions.create(model,messages,stream=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "def gpt_answer(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages =[\n",
    "            {\"role\" : \"system\", \"content\":gpt_system_prompt},\n",
    "            {\"role\" : \"user\" , \"content\": question}\n",
    "            ],\n",
    "        stream = True\n",
    "        )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id = True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        response = response.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "        update_display(Markdown(response),display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "789c8b10-288b-472c-8ba4-1b6ceb7e93f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a brief explanation of the code you've provided:\n",
       "\n",
       "| **Component**                | **Description**                                                                                                                                      |\n",
       "|------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
       "| `openai`                     | Refers to the OpenAI library, which is used to interact with OpenAI's models, such as ChatGPT.                                                      |\n",
       "| `chat.completions.create`    | This is a method that generates a chat completion from the model based on provided messages.                                                          |\n",
       "| `model`                      | This is a parameter where you specify the AI model to use (e.g., `\"gpt-3.5-turbo\"`). It determines how the AI will respond to the messages given.  |\n",
       "| `messages`                   | This parameter takes a list of messages (text) that represent the conversation history, which the model uses to generate a relevant response.       |\n",
       "| `stream=True`                | This option enables streaming of the response. Instead of waiting for a complete response, you receive parts of the response incrementally as they are generated. |\n",
       "\n",
       "### Purpose of the Code:\n",
       "The code is primarily used to obtain responses from an OpenAI model in a chat format, allowing for real-time interaction. The 'stream=True' option is useful for applications where you want to display the output progressively, enhancing the user experience."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt_answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aaf97e4-451b-4d05-a209-f13fc549d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_system_prompt = \"You are a personal study tutor, designed to provide clear, yet brief and succint answers to \\\n",
    "students that ask you questions. The topics are related to data science, computer science \\\n",
    "and technology in general, so you are allowed to use a moderate level of jargon. Explain in \\\n",
    "simple terminology, so a student can easily understand.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "import ollama\n",
    "\n",
    "def llama_answer(question):\n",
    "    response = ollama.chat(\n",
    "                model=MODEL_LLAMA,\n",
    "                messages=[\n",
    "                {\"role\" : \"system\", \"content\":llama_system_prompt},\n",
    "                {\"role\" : \"user\" , \"content\": question}\n",
    "                ],\n",
    "            )\n",
    "    return response['message']['content']\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac8a300-f767-413c-94ea-372651d5d8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me break it down for you.\\n\\n**What is OpenAI Chatcompletions?**\\nOpenAI Chatcompletions is a service provided by the company OpenAI that allows users to generate human-like text based on a given prompt or input. It\\'s like having a conversational AI assistant, but without the need for direct interaction.\\n\\n**The `create` method**\\nThe code snippet you provided uses this service to create a new instance of a model (a pre-trained language generation model). The `create` method takes three parameters:\\n\\n1. **Model**: This is the name of the specific OpenAI Chatcompletions model you want to use. Think of it like choosing a \"template\" for generating text.\\n2. **Messages**: These are the input messages that will be used to train or fine-tune the model. The goal is to provide enough context for the model to understand what kind of output is desired.\\n3. **Stream=True**: This is an optional parameter (but it\\'s set to `True` in your example) that enables streaming mode. In this mode, the API returns a response as you send each input message, rather than waiting for all messages to be sent before providing feedback.\\n\\n**What does the code do?**\\nWhen you run this code:\\n\\n* It creates a new instance of the specified OpenAI Chatcompletions model.\\n* It uses the provided `messages` to train or fine-tune the model.\\n* If streaming mode is enabled (`stream=True`), it provides instant feedback and response as you send each input message.\\n\\nIn summary, this code is used to create an interactive text generation conversation using a pre-trained language model from OpenAI.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_answer(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
